#summary Crawling XML Files

A [http://en.wikipedia.org/wiki/Web_crawler WebCrawler] takes url of html document as input. It parses the html document and finds the resources referred by `<a href="...">` in that document. It repeats the same process on the html resources referred and so on. While doing this it saves the resources into local filesystem.

Similarly `jlibs.xml.sax.crawl.XMLCrawler` is for XML Files.

However an xml document can refer to another xml document in many ways. for example:

XMLSchema uses `<xsd:import>` and `<xsd:include>`<br>
WSDL uses `<wsdl:import`, `<wsdl:include>`, `<xsd:import>` and `<xsd:include>`

i.e each type of xml document has its own way of referring other xml documents.

you need to tell `XMLCrawler`, what type links are expected in a particular xml document.

There are preconfigured subclasses of 'XMLCrawler`
{{{
jlibs.xml.xsd.crawl.XSCrawler    // for XMLSchema Documents
jlibs.xml.wsdl.crawl.WSDLCrawler // for WSDL Documents
jlibs.xml.xsl.crawl.XSLCrawler   // for XML StyleSheets
}}}

*Usage:*

{{{
import jlibs.xml.wsdl.crawl.WSDLCrawler;

String dir = "d:\\crawl"; // directory where to save crawled documents
String wsdl = "https://fps.amazonaws.com/doc/2007-01-08/AmazonFPS.wsdl"; // wsdl to be crawled

new WSDLCrawler().crawlInto(new InputSource(wsdl), new File(dir));
}}}

All xml documents are saved into the specified directory. After running above code, you will find following files in `d:\crawl` directory
{{{
AmazonFPS.wsdl
AmazonFPS.xsd
}}}
It never overwrites any existing file in that directory. So if you run the above code twice, you will see following files in `d:\crawl` directory
{{{
AmazonFPS1.wsdl
AmazonFPS1.xsd
AmazonFPS.wsdl
AmazonFPS.xsd
}}}

you could also do:
{{{
new WSDLCrawler().crawl(new InputSource(wsdl), new File("d:\\crawl\\target.wsdl"));
}}}

`crawl(...)` method's second argument is the file where to save the document specified in first argument. It will save all referred documents in the containing directory of second argument.
for example, the above creates following files in `d:\crawl`
{{{
target.wsdl
AmazonFPS.xsd
}}}

*NOTE:* All files are saved directly in given directory, i.e, no subdirectories are created.
-------------------------------
`XMLCrawler` is not an abstract class. You can use it but needs to be configured before crawling.

XSLCrawler, WSDLCrawler and XSCrawler are just preconfigured subclasses provided in jlibs for ease of use;

Let us see how to configure XMLCrawler for XMLSchema Documents.

{{{
import jlibs.xml.sax.crawl.XMLCrawler;
import jlibs.xml.sax.crawl.AttributeLink;
import jlibs.xml.Namespaces;

XMLCrawler crawler = new XMLCrawler();

// first define all possible links

/* /xsd:schema/xsd:import/@schemaLocation */
AttributeLink xsImport = new AttributeLink("schemaLocation", "xsd");
xsImport.pushElement(Namespaces.URI_XSD, "schema");
xsImport.pushElement(Namespaces.URI_XSD, "import");

/* /xsd:schema/xsd:include/@schemaLocation */
AttributeLink xsInclude = new AttributeLink("schemaLocation", "xsd");
xsInclude.pushElement(Namespaces.URI_XSD, "schema");
xsInclude.pushElement(Namespaces.URI_XSD, "include");

// now add link definitions to crawler
crawler.addLink(xsImport);
crawler.addLink(xsInclude);

// now crawler is ready for use
String xsd = "http://somesite.com/xsds/complex.xsd";
String dir = "d:\\crawl";
crawler.crawlInto(new InputSource(xsd), new File(dir), "xsd"); // third argument is possible extensions
}}}

The last argument of following is `String... extensions`
{{{
new AttributeLink(...)
XMLCrawler.crawlInto(...)
XMLCrawler.crawl(...)
}}}

why does it require extensions at all.

XMLCrawler can guess the extension for given URL, to its best. But for example:
{{{
<wsdl:import namespace="http://mywebsite.com" schemaLocation="http://mywebsite.com/samle/test"/>
<wsdl:import namespace="http://mywebsite.com" schemaLocation="http://mywebsite.com/samle/test.cgi"/>
}}}
in first case, `XMLCrawler` has no knowlege of what extension to use;<br>
in second case, `XMLCrawler` guesses the extension as `cgi`, which is wrong

in first case, 'XMLCrawler` will use the first extension among the extensions given.<br>
in second case, it tests whether guessed extension `cgi` is a valid one by checking whether possible extensions given has `cgi`

*AttributeLink:*

'XMLCrawler` simply does its job of crawling. It has no knowledge of
 * whether a particular element's attribute is a link
 * how to resolve a link
 * to crawl or not, a particular link
 * if has to crawl, where should I save it.
 * how to correct a link to refer to locally saved document

It is `AttributeLink` which decides all of the above. So you can override `AttributeLink` methods to suit your needs.

For this we first need to understand how `AttributeLink` manages all above tasks.

This post is not complete. will be finishing soon
